[{"content":"Status: [Latest]\nChip Huyen, a Vietnamese writer, computer scientist, and co-founder of Claypot AI, has an impressive background as a lecturer at Stanford and as a machine learning engineer at NVIDIA, Snorkel AI, Netflix, and Primer. I have been an avid reader of Chip\u0026rsquo;s books since her best-selling works in Vietnam, which inspired me to pursue my study-abroad journey in the U.S. Her latest book, published in June 2022, offers invaluable insights and practical advice on building reliable machine learning systems. After reading it for the first time, I was captivated by the depth of knowledge shared, prompting me to revisit the book multiple times to fully absorb its ideas. In this blog post, I will share my reflections and takeaways from Chip Huyen\u0026rsquo;s book after the first-read, which has greatly influenced my understanding and approach to machine learning.\nFig. 1. Book cover. (Image source: O'Reilly). Overview of ML Systems In her book, Chip Huyen presents an overview of ML systems, defining eight main components that constitute such systems. The book adopts a \u0026ldquo;divide-and-conquer\u0026rdquo; approach, examining each component individually to facilitate a comprehensive understanding of the entire system.\nFig. 2. Machine learning system components. (Image inspired from: O'Reilly). Chip\u0026rsquo;s definition of machine learning revolves around learning complex patterns from existing data and using them to make predictions on unseen data. This definition aligns with the prevalent use of supervised learning, as most ML applications involve leveraging vast amounts of user data stored in SQL databases.\nSo why does company integrate ML in their product? Companies integrate ML into their products due to the low cost of wrong predictions, scalability advantages, and the ability of ML models to adapt to changing patterns. For example, by identifying potential customers, displaying targeted ads, and offering timely discounts, companies like Lyft can significantly increase profits by reducing costs and attracting more long-term users.\nSo what is the difference between traditional software vs. ML? There are notable differences between traditional software engineering (SWE) and ML systems. In SWE, the emphasis is on modular and separated components, whereas in ML systems, code and data intertwine, making it challenging to apply SWE principles such as S.O.L.I.D or the 12-factor app to these systems.\nFig. 3. SWE vs. ML. (Image inspired from: O'Reilly). Considerations Before applying ML algorithms to solve a problem, it is essential to frame the problem in a way that ML can address it effectively. This involves determining the appropriate task for ML, such as binary classification, multi-class classification, or regression.\nWhile ML metrics like accuracy or F1 score are important, most companies prioritize the ultimate goal of any project: increasing profit. This can be achieved by boosting sales, reducing costs, enhancing customer satisfaction, or increasing app engagement.\nFig. 4. Integration in SWE or ML need to be align with business. (Image source: MonkeyUser.com). Predicting ad click-through rates and fraud detection are popular ML use cases because they directly impact business metrics. Higher click-through rates translate into increased ad revenue, while detecting and preventing fraudulent transactions saves money.\nData \u0026amp; Feature Engineering Let\u0026rsquo;s look at how data can be handled in the data science perspective. For now, I will focus on sampling, labeling, and feature engineering techniques.\nData Sampling To create train/validate/test sets, there are various data sampling techniques available. Two common approaches include random sampling and stratified sampling.\nData Labeling When it comes to data labeling, there are several options to consider:\nHand-labeling: This approach can be expensive, time-consuming, and may raise concerns about data privacy. Third-party service: Companies like Scale AI offer data labeling services and have recently secured significant contracts, such as the $250 million data labeling contract from the Department of Defense. Benchmark datasets: Utilizing established benchmark datasets like ImageNet and BATADAL can provide pre-labeled data for specific tasks. Methodological changes: If feasible, it's possible to switch from traditional supervised learning with training from scratch to alternative approaches like transfer learning, self-supervised learning, or semi-supervised learning. These methods can reduce the reliance on extensive labeled data. Fig. 5. Finding a suitable dataset and processing it for specific task is crucial for intended application purposes. (Image source: MonkeyUser.com). Feature Engineering The significance of feature engineering cannot be overstated in the development of ML models. Even with state-of-the-art ML architectures, performance can suffer if an inadequate set of features is used. While (deep learning) models can extract features to some extent, providing well-engineered features that enable the model to better comprehend the data can greatly benefit your application. By inputting carefully selected and curated features, you enhance the model\u0026rsquo;s ability to effectively capture and utilize relevant information.\nML Algorithms \u0026amp; Evaluation Let\u0026rsquo;s see what is Chip\u0026rsquo;s tips in terms of suitable ML algorithm(s), training paradigm, and evaluation metrics.\nModel Selection Given the constraints of limited computation power and time, it is crucial to strategically choose ML models based on their advantages and disadvantages. This is where the knowledge gained from classes and academic papers becomes applicable. Benchmark datasets, such as ImageNet, SMAC, or VQA, can serve as valuable references for model selection. Additionally, factors like the number of parameters (model complexity) and interpretability can influence the choice of models. The availability of auto-selecting tools like AutoML from H2O.ai or AutoTrain from HuggingFace provides reasonable options as well.\nDistributed Training Paradigms Consideration of distributed training paradigms is important when dealing with large models or datasets. Let\u0026rsquo;s examine data parallelism and model parallelism.\nData parallelism involves splitting the data across multiple machines, training the model on each machine, and accumulating gradients.\nModel parallelism, on the other hand, involves training different components of the model on different machines. For instance, machine 0 handles the computation for the first two layers while machine 1 handles the next two layers. It\u0026rsquo;s important to note that \u0026ldquo;model parallelism\u0026rdquo; can be misleading because in some cases, parallel execution doesn\u0026rsquo;t occur across different parts of the model on different machines. Instead, each part is executed sequentially or consecutively.\nFig. 6. Data parallelism vs. model parallelism. (Image source: anyscale.com). Model Evaluation Deploying a model involves more than simply pickling the model and using it in a Python script. There are several important aspects to consider in the deployment process.\nModel Deployment Model deployment is not simply \u0026ldquo;pickle\u0026rdquo; the model and use it on a Python script, there are several aspects to be considered.\nBatch vs. Online Training Online prediction is when predictions are generated and returned as soon as requests for these predictions arrive. For example, you enter an English sentence into Google Translate and get back its French translation immediately. When doing online prediction, requests are sent to the prediction service via RESTful APIs, that means to send back the input data, call model prediction and return the prediction result\nBatch prediction is when predictions are generated periodically or whenever triggered. The predictions are stored somewhere, such as in SQL tables or an in-memory database, and retrieved as needed. For example, Netflix might generate movie recommendations for all of its users every four hours, and the precomputed recommendations are fetched and shown to users when they log on to Netflix. For example, every 4 hours, Netflix might generate movie recommendations for all of its users.\nModel Compression In scenarios where convolutional neural networks are used, model compression techniques can be applied. For example, replacing the convolutional layer with depth-wise separable convolutional layers can significantly reduce the number of parameters while maintaining performance. Similarly, classical ML algorithms like Decision Trees can be pruned to reduce model complexity.\nCloud vs. Hardware Model deployment options include cloud deployment to platforms like AWS or GCP and hardware deployment, often referred to as edge computing. Cloud deployment offers scalability but comes with associated costs ranging from $50K to $2M per year. On the other hand, hardware deployment enables models to operate in environments with limited or unreliable internet connectivity, such as rural areas or developing countries. Edge computing also reduces network latency concerns. However, privacy risks still exist, as attackers could potentially steal user data by physically accessing the devices. Major companies like Google, Apple, and Tesla are actively developing their own chips optimized for edge computing.\nFig. 7. Google's Pixel 6 Tensor Chip, Apple's M1 Chip, Tesla's D1 Chip. In support of the semiconductor industry, the CHIPS and Science Act was signed by President Biden on August 9, 2022, aiming to strengthen the US semiconductor supply chain and foster research and development of advanced technologies within the country.\nInfrastructure The right infrastructure setup can automate processes, saving engineering time, accelerating ML application development and delivery, reducing the risk of bugs, and enabling new use cases. Conversely, a poorly implemented infrastructure can be cumbersome and costly to replace.\nFig. 8. ML layers of tools to build reliable infrastructure. Storage: is where data is collected and stored. It can range from simple setups like hard disk drives (HDD) or solid-state drives (SSD) to more sophisticated solutions like centralized storage in platforms such as Amazon S3 or Snowflake. Data can be stored in a single location or distributed across multiple locations.\nResource Management: comprises tools to schedule and orchestrate your workloads to make the most out of your available compute resources. Popular examples in this category include Airflow, Kubeflow, and Metaflow, which help manage and allocate computing resources efficiently.\nML Platform: provides tools to aid the development of ML applications such as model stores, feature stores, and monitoring tools. Prominent examples of ML platforms include SageMaker and MLflow, which provide comprehensive capabilities for ML application development and management.\nDeployment: Major cloud providers like AWS (SageMaker), GCP (Vertex AI), Azure (Azure ML), and Alibaba (Machine Learning) offer robust deployment options for ML applications. These platforms provide infrastructure and services to streamline the deployment process and ensure scalability and reliability.\nDevelopment Environment: A variety of development environments are available to facilitate ML application development. Tools such as VS Code, PyCharm, Anaconda, Jupyter Notebook, Git, and Docker are commonly used by data scientists and developers to write and manage code, create reproducible environments, and collaborate efficiently.\nTeam Structures In many job descriptions, it is common to see Data Scientists taking ownership of the entire process, from data collection to productionization. However, Netflix has implemented an effective team structure for full-stack data scientists.\nNetflix\u0026rsquo;s model involves specialists who initially own specific parts of a project. These specialists create tools and automation to streamline their respective areas. Data scientists can then leverage these tools to take ownership of their projects from end to end. As tools are developed and integrated with each other, the entire workflow is cohesive and well-integrated.\nThis team structure at Netflix allows for a seamless collaboration where specialists contribute their expertise to build tools, and data scientists can utilize these tools to manage the complete life cycle of their projects.\nFig. 9. Netflix's full cycle data science development with specialists create reusable tools. (Image source: Netflix Technology Blog). Citation Cited as:\nNguyen, Minh. (May 2023). Designing Machine Learning Systems: A Summary. https://mnguyen0226.github.io/posts/ml_systems_design/post/. Or\n@article{nguyen2023mlsys, title = \u0026#34;Designing Machine Learning Systems: A Summary.\u0026#34;, author = \u0026#34;Nguyen, Minh\u0026#34;, journal = \u0026#34;mnguyen0226.github.io\u0026#34;, year = \u0026#34;2023\u0026#34;, month = \u0026#34;May\u0026#34;, url = \u0026#34;https://mnguyen0226.github.io/posts/ml_systems_design/post/\u0026#34; } References [1] “Full cycle developers at Netflix,” Medium, https://netflixtechblog.com/full-cycle-developers-at-netflix-a08c31f83249 (accessed May 19, 2023).\n[2] “President Biden Signs Chips and science act into law,” White \u0026amp; Case LLP, https://www.whitecase.com/insight-alert/president-biden-signs-chips-and-science-act-law (accessed May 19, 2023).\n[3] “Cloud cost management and optimization by ANODOT,” Anodot, https://www.anodot.com/cloud-cost-management/ (accessed May 19, 2023).\n[4] F. Chollet, “Xception: Deep learning with depthwise separable convolutions,” arXiv.org, https://arxiv.org/abs/1610.02357 (accessed May 19, 2023).\n[5] C. Huyen, “Introduction to streaming for Data scientists,” Chip Huyen, https://huyenchip.com/2022/08/03/stream-processing-for-data-scientists.html#:~:text=Batch%20prediction%20means%20periodically%20generating,whenever%20they%20visit%20the%20website. (accessed May 19, 2023).\n[6] Y. Wu et al., “Google’s Neural Machine Translation System: Bridging the gap between human and machine translation,” arXiv.org, https://arxiv.org/abs/1609.08144 (accessed May 19, 2023).\n[7] “What is distributed training?,” Anyscale, https://www.anyscale.com/blog/what-is-distributed-training (accessed May 19, 2023).\n[8] VQA: Visual question answering | IEEE conference publication - IEEE xplore, https://ieeexplore.ieee.org/document/7410636 (accessed May 19, 2023).\n[9] B. Ellis et al., “SMACv2: An improved benchmark for cooperative multi-agent reinforcement learning,” arXiv.org, https://arxiv.org/abs/2212.07489 (accessed May 19, 2023).\n[10] Practical lessons from predicting clicks on ads at facebook, https://quinonero.net/Publications/predicting-clicks-facebook.pdf (accessed May 19, 2023).\n[11] “Scale AI selected by U.S. Department of Defense to accelerate government’s AI capabilities,” Business Wire, https://www.businesswire.com/news/home/20220131005304/en/Scale-AI-Selected-by-U.S.-Department-of-Defense-to-Accelerate-Government%E2%80%99s-AI-Capabilities (accessed May 19, 2023).\n[12] “Deep learning with python,” Manning Publications, https://www.manning.com/books/deep-learning-with-python (accessed May 19, 2023).\n[13] J. Henriksen, “Valuing lyft requires a deep look into unit economics,” Forbes, https://www.forbes.com/sites/jeffhenriksen/2019/05/17/valuing-lyft-requires-a-deep-look-into-unit-economics/?sh=17155c9a7add (accessed May 19, 2023).\n[14] L. Ceci and J. 7, “Mobile app user acquisition cost 2019,” Statista, https://www.statista.com/statistics/185736/mobile-app-average-user-acquisition-cost/ (accessed May 19, 2023).\n[15] C. Huyen, “Designing machine learning systems,” O’Reilly Online Learning, https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/ (accessed May 19, 2023).\nFig. 10. McAfee Knob, Roanoke, Virginia, U.S.A. ","permalink":"https://mnguyen0226.github.io/posts/ml_systems_design/post/","summary":"Status: [Latest]\nChip Huyen, a Vietnamese writer, computer scientist, and co-founder of Claypot AI, has an impressive background as a lecturer at Stanford and as a machine learning engineer at NVIDIA, Snorkel AI, Netflix, and Primer. I have been an avid reader of Chip\u0026rsquo;s books since her best-selling works in Vietnam, which inspired me to pursue my study-abroad journey in the U.S. Her latest book, published in June 2022, offers invaluable insights and practical advice on building reliable machine learning systems.","title":"Designing Machine Learning Systems: A Summary"},{"content":"Status: [Latest]\nI recently finished an excellent graduate course, Software Engineering (CS5704), and learned about different aspects of software projects and how different-size companies handle their technical/business changes to deliver successful products to their customer. Some important topics are Process Models (Waterfall, V-Model, Spiral, Agile), Requirements Definition, and Architecture Design Patterns. Especially, S.O.L.I.D principles have struck me as must-known concepts for writing better and cleaner code.\nWhy do S.O.L.I.D principles matter? According to Uncle Bob, bad code slows down the development team as it is confusing and fragile. Confusing code does not explain what it is doing, while fragile code breaks in many places when you change one or a few lines of code.\nWhat we want is the code that is clear, rigid, and reusable.\nFun Fact: In his talk in S.O.L.I.D Principles, he mentioned that he was not the first to realize or coin the acronym \u0026ldquo;SOLID.”\nSingle Responsibility Principle SRP: \u0026ldquo;A class should have one, and only one reason to change\u0026rdquo; — Robert C. Martin. In Object Oriented Programming (OOP), a class should have only one primary function. If there is more than one utility for that class, we should split it into multiple courses. This helps distribute the functional responsibilities across numerous classes or objects (or developers).\nFig. 1. Don't let your code (or yourself) take on all the responsibilties in the project. Divide and conquer! (Image source: MonkeyUser.com). Web Development Example Let\u0026rsquo;s say you are a developer in the Google Shopping team who is in charge of designing a class to process post-item-purchase.\nCode Example class PostPurchaseProcess: def web_notification(self, message): \u0026#34;\u0026#34;\u0026#34; Code to send confirmation bill to Google Chrome \u0026#34;\u0026#34;\u0026#34; pass def email_notification(self, message): \u0026#34;\u0026#34;\u0026#34; Code to send confirmation bill to Gmail \u0026#34;\u0026#34;\u0026#34; pass def phone_notification(self, message): \u0026#34;\u0026#34;\u0026#34; Code to send confirmation bill to Google Pixel \u0026#34;\u0026#34;\u0026#34; pass In the example above, the class PostPurchaseProcess violates the SRP as it contains too many responsibilities: sending notifications to web app, email, and mobile. What if there are errors in sending notifications to Google Pixel and Gmail? It may take time to pinpoint precisely which function(s) is responsible for the mistake!\nFix: We can refactor the code into multiple classes, each with single responsibility.\nFig. 2. Design refactored with single responsibility principle. Code Example class WebPurchaseProcess: def web_notification(self, message): \u0026#34;\u0026#34;\u0026#34; Code to send confirmation bill to Google Chrome \u0026#34;\u0026#34;\u0026#34; pass class EmailPurchaseProcess: def email_notification(self, message): \u0026#34;\u0026#34;\u0026#34; Code to send confirmation bill to Gmail \u0026#34;\u0026#34;\u0026#34; pass class PhonePurchaseProcess: def phone_notification(self, message): \u0026#34;\u0026#34;\u0026#34; Code to send confirmation bill to Google Pixel \u0026#34;\u0026#34;\u0026#34; pass Data Science Example Suppose you are a Data Scientist at C3.ai who is processing a tabular dataset for a supervised classification application. There are multiple steps to investigate the structure, quality, and content of the dataset, such as: checking datatypes, removing duplicates, data imputation, removing outliers, class-balancing, feature analysis, or feature engineering. Similar to the Google Shopping example above, to follow the SRP, we will need to put these steps into their separate class.\nCode Example class DataImputation: def median_fill(self, data): \u0026#34;\u0026#34;\u0026#34; Code to fill missing data by median of feature \u0026#34;\u0026#34;\u0026#34; pass class OutlierRemoval: def remove_by_euclidean_dist(self, data): \u0026#34;\u0026#34;\u0026#34; Code to remove outliers using Euclidean distance \u0026#34;\u0026#34;\u0026#34; pass class FeatureAnalysis: def pearson_corr_cal(self, data): \u0026#34;\u0026#34;\u0026#34; Code to calculate Pearson correlation scores between input feature(s) and output label(s) \u0026#34;\u0026#34;\u0026#34; pass However, I don\u0026rsquo;t find this principle helpful in a Kaggle or data science project, as each Jupyter Notebook cell can be run and tested individually.\nOpen-Closed Principle OCP: \u0026ldquo;A module (or component) should be open for extension but closed for modification\u0026rdquo; — Bertrand Meyer. When making changes, the principle prevents the already functional design from bugs or breaks. This principle promotes a modular and flexible design that allows for the easy integration of a new idea while making your codebase more maintainable and scalable. The OCP can be hard to understand, so let\u0026rsquo;s walk through some examples.\nFig. 3. Sometime it is hard to implement new features without refactor the code. (Image source: MonkeyUser.com). Web Development Example An easy-to-see symptom of OCP violation to look for is the use of if/elif/else or switch-case statements. Let\u0026rsquo;s say that you are a (Flask or Django) backend developer at Meta who is writing a REST API that uses HTTP request protocols (POST, GET, PUT, DELETE) to allow users to interact with the database via CRUD (Create, Read, Update, Delete).\nCode Example class RequestHandler: def handle_request(self, request): \u0026#34;\u0026#34;\u0026#34; Code to handle request based on type \u0026#34;\u0026#34;\u0026#34; if request.method == \u0026#34;GET\u0026#34;: self.handle_get(request) elif request.method == \u0026#34;POST\u0026#34;: self.handle_post(request) elif request.method == \u0026#34;PUT\u0026#34;: self.handle_put(request) elif request.method == \u0026#34;DELETE\u0026#34;: self.handle_delete(request) def handle_get(self, request): \u0026#34;\u0026#34;\u0026#34; Code to handle GET request \u0026#34;\u0026#34;\u0026#34; pass def handle_post(self, request): \u0026#34;\u0026#34;\u0026#34; Code to handle POST request \u0026#34;\u0026#34;\u0026#34; pass def handle_put(self, request): \u0026#34;\u0026#34;\u0026#34; Code to handle PUT request \u0026#34;\u0026#34;\u0026#34; pass def handle_delete(self, request): \u0026#34;\u0026#34;\u0026#34; Code to handle DELETE request \u0026#34;\u0026#34;\u0026#34; pass The example above violates OCP because every time a new request type is added (e.g., PATCH), our RequestHandler class needs to be modified. This can introduce new bugs into existing code. As the OCP stated, we should design our code to be fixed but extended.\nFix: A solution to OCP violation is to separate each request type into individual classes, thus abstracting the RequestHandler class. Therefore, if we want to add a PATCH request, we can extend our API by adding a new PatchRequestHander class.\nFig. 4. Design refactored with open-closed principle. Code Example class RequestHandler: def handle_request(self, request): \u0026#34;\u0026#34;\u0026#34; Code to handle request based on type \u0026#34;\u0026#34;\u0026#34; request.handle() class GetRequestHandler: def handle(self): \u0026#34;\u0026#34;\u0026#34; Code to handle GET request \u0026#34;\u0026#34;\u0026#34; pass class PostRequestHandler: def handle(self): \u0026#34;\u0026#34;\u0026#34; Code to handle POST request \u0026#34;\u0026#34;\u0026#34; pass class PutRequestHandler: def handle(self): \u0026#34;\u0026#34;\u0026#34; Code to handle PUT request \u0026#34;\u0026#34;\u0026#34; pass class DeleteRequestHandler: def handle(self): \u0026#34;\u0026#34;\u0026#34; Code to handle DELETE request \u0026#34;\u0026#34;\u0026#34; pass class PatchRequestHandler: def handle(self): \u0026#34;\u0026#34;\u0026#34; Code to handle PATCH request (extended) \u0026#34;\u0026#34;\u0026#34; pass Data Science Example Let\u0026rsquo;s say you are a Machine Learning Engineer at NVIDIA who is writing a Python script for a baseline model with data preprocessing, model training, and model evaluation. Like the REST API example above, you want your class ModelPipeline to remain closed for modification but open for extension by strictly following the OCP.\nCode Example class DataPreprocessor: def preprocess(self, dataset): \u0026#34;\u0026#34;\u0026#34; Code to preprocess data \u0026#34;\u0026#34;\u0026#34; pass class ModelTrainer: def train(self, dataset): \u0026#34;\u0026#34;\u0026#34; Code to train model \u0026#34;\u0026#34;\u0026#34; pass class ModelEvaluator: def evaluate(self, model, dataset): \u0026#34;\u0026#34;\u0026#34; Code to evaluate model \u0026#34;\u0026#34;\u0026#34; pass class ModelPipeline: def __init__(self, preprocessor, trainer, evaluator): \u0026#34;\u0026#34;\u0026#34; Code to build a ModelPipeline constructor (object) \u0026#34;\u0026#34;\u0026#34; self.preprocessor = preprocessor self.trainer = trainer self.evaluator = evaluator def run_pipeline(self, dataset): \u0026#34;\u0026#34;\u0026#34; Code to run the machine learning project end-to-end \u0026#34;\u0026#34;\u0026#34; preprocessed_data = self.preprocessor.preprocess(dataset) model = self.trainer.train(preprocessed_data) evaluation_result = self.evaluator.evaluate(model, preprocessed_data) return evaluation_result Your teammate develops a new way of processing the dataset. Luckily, due to your guideline of OCP, your teammate can easily extend the existing baseline model by inheriting the DataPreprocessor class without the risk of breaking your functional baseline design.\nCode Example class NewDataPreprocessor(DataPreprocessor): def preprocess(self, dataset): \u0026#34;\u0026#34;\u0026#34; Code to preprocess data with new technique \u0026#34;\u0026#34;\u0026#34; pass Liskov Substitution Principle LSP: \u0026ldquo;Subclasses should be substitutable for their base classes (without affecting the correctness of the program)\u0026rdquo; — Barbara Liskov. In OOP, what we want is for any method or code that works for a base class should continue to work correctly when used with the derived types. This principle ensures the inheritance hierarchies are consistent, extensible, and correct.\nFig. 5. Design your codebase to be more predictable for later reuse. (Image source: MonkeyUser.com). Web Development \u0026amp; Data Science Example Let\u0026rsquo;s say that you are to design a codebase at Netflix to write processed data into a MySQL database and a CSV file (You can think of adding new columns into a database by scraping or engineering new features).\nCode Example class DataHandler(): def write_db(self, data): \u0026#34;\u0026#34;\u0026#34; Code to write processed data to MySQL database \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Handling MySQL database.\u0026#34;) def write_csv(self, data): \u0026#34;\u0026#34;\u0026#34; Code to write processed data to CSV file \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Handling CSV file.\u0026#34;) class WriteDB(DataHandler): def write_db(self, data): \u0026#34;\u0026#34;\u0026#34; Code to write processed data to MySQL database \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Handling MySQL database.\u0026#34;) def write_csv(self, data): \u0026#34;\u0026#34;\u0026#34; Code to write processed data to CSV file \u0026#34;\u0026#34;\u0026#34; raise Exception(\u0026#34;Error: Can\u0026#39;t write to CSV file.\u0026#34;) class WriteCSV(DataHandler): def write_db(self, data): \u0026#34;\u0026#34;\u0026#34; Code to write processed data to MySQL database \u0026#34;\u0026#34;\u0026#34; raise Exception(\u0026#34;Error: Can\u0026#39;t write to MySQL database.\u0026#34;) def write_csv(self, data): \u0026#34;\u0026#34;\u0026#34; Code to write processed data to CSV file \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Handling CSV file.\u0026#34;) Here, our base class DataHandler defined two methods, write_db() and write_csv(). The derived class WriteDB inherits from DataHander and overrides the write_csv method. However, instead of providing the expected behavior, it prints an error message indicating it can\u0026rsquo;t write to a CSV file. Similarly, the derived class WriteCSV prints out the error message indicating it can\u0026rsquo;t write to MySQL file. This design violates the LSP as the derived classes do not behave as expected based on the contract defined by the DataHandler base class. The base class is designed to be too specific, thus causing its children to handle edge case(s) based on the characteristics of the child classes.\nFixed: Let\u0026rsquo;s write a more generic base class!\nFig. 6. Design refactored with liskov substitution principle. Code Example class DataHandler(): def write(self, data): \u0026#34;\u0026#34;\u0026#34; Code to handle processed data \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Handling data.\u0026#34;) class WriteDB(DataHandler): def write(self, data): \u0026#34;\u0026#34;\u0026#34; Code to write processed data to MySQL database \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Handling MySQL database.\u0026#34;) class WriteCSV(DataHandler): def write(self, data): \u0026#34;\u0026#34;\u0026#34; Code to write processed data to CSV file \u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Handling CSV file.\u0026#34;) If you have an object of type WriteDB or WriteCSV, you can safely use it wherever an object of type DataHandler is expected because they adhere to the same contract.\nInterface Segregation Principle ISP: \u0026ldquo;Many client-specific interfaces are better than one general purpose interface\u0026rdquo; — Robert C. Martin. If we have an extensive interface with many functions, the class implementing the interface might only use some defined functions! It is better to break up the interface into multiple smaller interfaces; then, we can inherit our class\u0026rsquo;s needed interface(s). This principle promotes code modularity, reduces unnecessary dependencies, and makes it easier to maintain/extend the existing codebase.\nFig. 7 . Extensive class can be hard to be refactored, inherited, or reused. (Image source: MonkeyUser.com). Web Development Example Let\u0026rsquo;s say you are a Full-stack Web Developer at Odoo who writes a simple web app with a home page and admin page.\nCode Example class WebPage: def render(self): \u0026#34;\u0026#34;\u0026#34; Code to render HTML of a generic web page \u0026#34;\u0026#34;\u0026#34; pass def save(self): \u0026#34;\u0026#34;\u0026#34; Code to save data of a generic web page into a database \u0026#34;\u0026#34;\u0026#34; pass def delete(self): \u0026#34;\u0026#34;\u0026#34; Code to delete data of a generic web page from a database \u0026#34;\u0026#34;\u0026#34; pass class HomePage(WebPage): def render(self): \u0026#34;\u0026#34;\u0026#34; Code to render HTML of home page \u0026#34;\u0026#34;\u0026#34; pass def save(self): \u0026#34;\u0026#34;\u0026#34; Code to save data of home page into a database \u0026#34;\u0026#34;\u0026#34; pass def delete(self): \u0026#34;\u0026#34;\u0026#34; Code to delete data of home page from a database \u0026#34;\u0026#34;\u0026#34; raise Exception(\u0026#34;Error: Only Admin can delete data from database.\u0026#34;) class AdminPage(WebPage) def render(self): \u0026#34;\u0026#34;\u0026#34; Code to render HTML of admin page \u0026#34;\u0026#34;\u0026#34; pass def save(self): \u0026#34;\u0026#34;\u0026#34; Code to save data of admin page into a database \u0026#34;\u0026#34;\u0026#34; pass def delete(self): \u0026#34;\u0026#34;\u0026#34; Code to delete data of admin page from a database \u0026#34;\u0026#34;\u0026#34; pass The example above violates the ISP because the children should not be forced to depend on the parent\u0026rsquo;s method(s) they do not use. While the AdminPage class inherits just fine, the HomePage class can\u0026rsquo;t use the delete() function. This creates an unnecessary dependency!\nFixed: It will be better to segregate the parent (WebPage) class into smaller interface(s) that meet the needs of each child. We can separate render(), save(), and delete() functions into multiple classes.\nFig. 8. Design refactored with interface segregation principle. Code Example class Renderable: def render(self): \u0026#34;\u0026#34;\u0026#34; Code to render HTML of a generic web page \u0026#34;\u0026#34;\u0026#34; pass class Savable: def save(self): \u0026#34;\u0026#34;\u0026#34; Code to save data of a generic web page into a database \u0026#34;\u0026#34;\u0026#34; pass class Deletable: def delete(self): \u0026#34;\u0026#34;\u0026#34; Code to delete data of a generic web page from a database \u0026#34;\u0026#34;\u0026#34; pass class HomePage(Renderable, Savable): def render(self): \u0026#34;\u0026#34;\u0026#34; Code to render HTML of home page \u0026#34;\u0026#34;\u0026#34; pass def save(self): \u0026#34;\u0026#34;\u0026#34; Code to save data of home page into a database \u0026#34;\u0026#34;\u0026#34; pass class AdminPage(Renderable, Savable, Deletable): def render(self): \u0026#34;\u0026#34;\u0026#34; Code to render HTML of admin page \u0026#34;\u0026#34;\u0026#34; pass def save(self): \u0026#34;\u0026#34;\u0026#34; Code to save data of admin page into a database \u0026#34;\u0026#34;\u0026#34; pass def delete(self): \u0026#34;\u0026#34;\u0026#34; Code to delete data of admin page from a database \u0026#34;\u0026#34;\u0026#34; pass Data Science Example Let\u0026rsquo;s say that you are a Data Scientist at LinkedIn who is working on a multimodal application. You are tasked to process image and text data. Like the web development example above, you want separate interfaces for each subtask and inherit related interfaces to process images and text accordingly.\nCode Example class DataLoader: def load(self, data): \u0026#34;\u0026#34;\u0026#34; Code to load data to notebook memory (entire dataset or as a generator) \u0026#34;\u0026#34;\u0026#34; pass class DuplicateRemoval: def remove_dup(self, data): \u0026#34;\u0026#34;\u0026#34; Code to remove duplicate images in the dataset \u0026#34;\u0026#34;\u0026#34; pass class DataAugmentation: def img_augment(self, data): \u0026#34;\u0026#34;\u0026#34; Code to augment images in the dataset \u0026#34;\u0026#34;\u0026#34; pass class DataSmoothing: def ts_smooth(self, data): \u0026#34;\u0026#34;\u0026#34; Code to smooth time-series dataset \u0026#34;\u0026#34;\u0026#34; pass class ProcessTimeSeriesDataset(DataLoader, DataSmoothing): def load(self, data): \u0026#34;\u0026#34;\u0026#34; Code to load data to notebook memory (entire dataset or as a generator) \u0026#34;\u0026#34;\u0026#34; pass def ts_smooth(self, data): \u0026#34;\u0026#34;\u0026#34; Code to smooth time-series dataset \u0026#34;\u0026#34;\u0026#34; pass class ProcessImageDataset(DataLoader, DuplicateRemoval, DataAugmentation): def load(self, data): \u0026#34;\u0026#34;\u0026#34; Code to load data to notebook memory (entire dataset or as a generator) \u0026#34;\u0026#34;\u0026#34; pass def remove_dup(self, data): \u0026#34;\u0026#34;\u0026#34; Code to remove duplicate images in the dataset \u0026#34;\u0026#34;\u0026#34; pass def img_augment(self, data): \u0026#34;\u0026#34;\u0026#34; Code to augment images in the dataset \u0026#34;\u0026#34;\u0026#34; pass Here, we have the defined separate interfaces for data load, removing duplicates, image augmentation, and time-series smoothing. The ProcessTimeSeriesDataset and ProcessImageDataset classes only implement (or inherit) the interfaces (or parent classes) that are relevant to them.\nDependency Inversion Principle DIP: \u0026ldquo;Depend on abstractions. Do not depend on concretions\u0026rdquo; — Robert C. Martin. The main idea is to decouple high-level modules from low-level modules by introducing abstractions as mediators. When integrating external dependencies, it is better to create wrapper(s) around them so that your code depends on the wrapper you make and not the details of the dependencies. This allows for better flexibility, as different implementations can be easily substituted without affecting the high-level modules. This principle promotes modularity and maintainability in codebase design.\nFig. 9. Create a wrapper and hide the details. Not everyone need to know all the nitty-gritty of the system (Image source: MonkeyUser.com). Web Development Example Let\u0026rsquo;s say you are a Mobile Developer at Apple who work on the payment integration aspect of Apple Music. Your task is to integrate Stripe Payment API into your backend codebase.\nCode Example class StripeProcessor: def process_payment(self, credit_cart_num): \u0026#34;\u0026#34;\u0026#34; Code to process payment via Stripe Payment API \u0026#34;\u0026#34;\u0026#34; pass class AppleMusic: def notify_payment(self, credit_cart_num): \u0026#34;\u0026#34;\u0026#34; Code to process payment in iOS backend \u0026#34;\u0026#34;\u0026#34; processor = StripeProcessor() processor.process_payment(credit_cart_num) The Apple Music example above violates DIP as the AppleMusic class directly depends on StripeProcessor class, a specific low-level implementation. Imagine that Stripe provides Stripe Payment API version 2.0, which has a massive change in multiple methods. Our AppleMusic (and any other class that uses the StripeProcessor object) will be broken. We will have to fix every single line that uses StripeProcessor\u0026rsquo;s methods.\nFixed: AppleMusic and StripeProcessor classes should depend on abstractions (or a wrapper for StripeProcessor) to avoid such catastrophe. In addition, we can easily swap the external API (e.g., Venmo Payment API) within the wrapper class by having a wrapper.\nFig. 10. Design refactored with dependency inversion principle. Code Example class PaymentProcessor: def process_payment(self, credit_cart_num): \u0026#34;\u0026#34;\u0026#34; Code to process payment via external API \u0026#34;\u0026#34;\u0026#34; pass class StripeProcessor(PaymentProcessor): def process_payment(self, credit_cart_num): \u0026#34;\u0026#34;\u0026#34; Code to process payment via Stripe Payment API \u0026#34;\u0026#34;\u0026#34; pass class VenmoProcessor(PaymentProcessor): def process_payment(self, credit_cart_num): \u0026#34;\u0026#34;\u0026#34; Code to process payment via Venmo Payment API \u0026#34;\u0026#34;\u0026#34; pass class AppleMusic: def __init__(self, processor: PaymentProcessor): \u0026#34;\u0026#34;\u0026#34; Code to build AppleMusic constructor (object) \u0026#34;\u0026#34;\u0026#34; self.processor = processor def notify_payment(self, credit_cart_num): \u0026#34;\u0026#34;\u0026#34; Code to process payment in iOS backend \u0026#34;\u0026#34;\u0026#34; self.processor.process_payment(credit_cart_num) Data Science Example Let\u0026rsquo;s say you are a Data Analyst at Deloitte who is in charge of plotting the dataset to show insight to stakeholders. Similarly to the Apple Music example above, creating a wrapper for the data visualization task would be best.\nCode Example class Plotter: def show_plots(self, dataset): \u0026#34;\u0026#34;\u0026#34; Code to plot the dataset \u0026#34;\u0026#34;\u0026#34; pass class SeabornPlotter(Plotter): def show_plots(self, dataset): \u0026#34;\u0026#34;\u0026#34; Code to plot the dataset bia Seaborn API \u0026#34;\u0026#34;\u0026#34; pass class PlotlyPlotter(Plotter): def show_plots(self, dataset): \u0026#34;\u0026#34;\u0026#34; Code to plot the dataset bia Plotly API \u0026#34;\u0026#34;\u0026#34; pass class BusinessInsight(): def __init__(self, plotter: Plotter): \u0026#34;\u0026#34;\u0026#34; Code to build BusinessInsight constructor (object) \u0026#34;\u0026#34;\u0026#34; self.plotter = plotter def notify_payment(self, dataset): \u0026#34;\u0026#34;\u0026#34; Code to show the visualization trends or patterns of the dataset \u0026#34;\u0026#34;\u0026#34; self.plotter.show_plots(dataset) Here, the BusinessInsight class depends on the Plotter abstraction through its constructor, allowing different plotting implementations to be injected without modifying the BusinessInsight class. The Plotter class serves as the abstraction, while SeabornPlotter and PlotlyPlotter are concrete implementations of the Plotter class. Depending on the abstraction (Plotter), the BusinessInsight class is decoupled from specific plotting implementations. This promotes flexibility and modularity, as different plotting libraries or variations can be used interchangeably by appropriately implementing the Plotter abstraction to the BusinessInsight class.\nCitation Cited as:\nNguyen, Minh. (May 2023). S.O.L.I.D Principles Explained. https://mnguyen0226.github.io/posts/solid_principles/post/. Or\n@article{nguyen2023solid, title = \u0026#34;S.O.L.I.D Principles Explained.\u0026#34;, author = \u0026#34;Nguyen, Minh\u0026#34;, journal = \u0026#34;mnguyen0226.github.io\u0026#34;, year = \u0026#34;2023\u0026#34;, month = \u0026#34;April\u0026#34;, url = \u0026#34;https://mnguyen0226.github.io/posts/solid_principles/post/\u0026#34; } References [1] R. S. Pressman and B. R. Maxim, Software Engineering: A Practitioner’s Approach. New York, NY: McGraw-Hill Education, 2020.\n[2] R. C. Martin, M. C. Feathers, T. R. Ottinger, and J. J. Langr, Clean Code A Handbook of Agile Software Craftsmanship. Boston, MA: Pearson Education, Inc, 2016.\n[3] R. C. Martin, “Clean Code - Lecture Series,” YouTube, https://www.youtube.com/watch?v=7EmboKQH8lM\u0026amp;amp;list=PLwAjnlpkQEft41G-GvHAKnh_CkaEKFawh\u0026amp;amp;ab_channel=UnityCoin (accessed May 12, 2023).\n[4] “SOLID Design Principle - Web Dev Simplified,” YouTube, https://www.youtube.com/watch?v=UQqY3_6Epbg\u0026amp;amp;list=PLZlA0Gpn_vH9kocFX7R7BAe_CvvOCO_p9\u0026amp;amp;ab_channel=WebDevSimplified (accessed May 12, 2023).\nFig. 11. Ho Chi Minh City, Viet Nam. (Image source: Peter Nguyen @ Unsplash). ","permalink":"https://mnguyen0226.github.io/posts/solid_principles/post/","summary":"Status: [Latest]\nI recently finished an excellent graduate course, Software Engineering (CS5704), and learned about different aspects of software projects and how different-size companies handle their technical/business changes to deliver successful products to their customer. Some important topics are Process Models (Waterfall, V-Model, Spiral, Agile), Requirements Definition, and Architecture Design Patterns. Especially, S.O.L.I.D principles have struck me as must-known concepts for writing better and cleaner code.\nWhy do S.O.L.I.D principles matter?","title":"S.O.L.I.D Principles Explained"},{"content":"Status: [Latest]\nDuring a recent interview with a Data Science Lead at a digital agriculture tech company, I had the opportunity to delve into the machine learning team\u0026rsquo;s exciting project. The team was utilizing multi-task learning (MTL) to deploy models to farming hardware, which piqued my interest and prompted me to explore this approach combined with deep learning. In particular, I wanted to investigate the effectiveness of MTL on vision transformers (ViT) and deep residual networks (ResNet-152).\nIn recent years, MTL has gained significant attention as a powerful technique to tackle multiple tasks simultaneously while optimizing computational resources. In computer vision, MTL has shown great potential in addressing challenges such as image segmentation, key-point detection, and edge detection. It has demonstrated remarkable improvements in data efficiency and performance on related tasks. Notably, Andrej Karpathy\u0026rsquo;s work on \u0026ldquo;Tesla Autopilot and Multi-Task Learning for Perception and Prediction\u0026rdquo; highlights how MTL enables the deployment of large models in constrained hardware settings while improving task-specific performance.\nFig. 1. Multi-Task Learning makes it possible to deploy large model in constraint settings. (Image source: Andrej Karpathy @ Tesla). Motivated by these advancements, my goal is to thoroughly investigate the effectiveness of MTL using the ViT architecture for image classification. Additionally, I aim to compare its performance against single-task learning (STL). I focus on class and super-class classification tasks extracted from the popular CiFAR-10 and CiFAR-100 datasets to conduct this investigation. I leverage the power of Python and Tensorflow in implementing and evaluating these experiments. For the convenience of interested readers, I have made the code and results of this research available in my Github repository.\nMulti-Task Learning Multi-Task Learning (MTL) is a powerful technique that allows networks to learn multiple related tasks simultaneously instead of training separate models for each task. MTL offers better efficiency and generalization than Single-Task Learning (STL), making it popular in various fields.\nEfficiency is crucial in embedded applications and deployment, where hardware limitations and cloud storage costs are considerations. MTL optimizes computational resources by jointly learning multiple tasks in a single model, improving efficiency and reducing complexity.\nGeneralization is essential for building artificial generalized intelligence. MTL leverages shared representations to gain a broader understanding of data patterns and correlations, enhancing adaptability to diverse scenarios.\nThere are two main approaches to implementing MTL: \u0026ldquo;hard-parameter\u0026rdquo; sharing and \u0026ldquo;soft-parameter\u0026rdquo; sharing. Hard-parameter sharing involves sharing some or all of the layers between tasks, enabling efficient knowledge transfer and improving model performance. In this post, I explore the application of hard-parameter sharing in image classification, specifically using vision transformers (ViT) and deep residual networks (ResNet-152).\nFig. 2. Hard-parameter sharing vs soft-parameter sharing. (Image inspired from The Gradient). There are three research questions that I want to tackle:\nRQ1: Is an MTL ViT model superior in performance to an MTL Convolution-based model (ResNet-152)? RQ2: Can an MTL ViT model achieve better results than two separate STL ViT models? RQ3: Does an STL ViT model outperform an STL Convolution-based model (ResNet152) in terms of accuracy? CiFAR-10 \u0026amp; CiFAR-100 Datasets For the CiFAR-10 dataset, I have selected two tasks: Task 1 involves a 10-class classification, while Task 2 focuses on binary classification by categorizing the ten classes into \u0026ldquo;animal\u0026rdquo; or \u0026ldquo;vehicle\u0026rdquo; labels. For the CiFAR-100 dataset, I have identified two tasks: Task 1 encompasses a 100-class classification, and Task 2 involves a 20-superclass classification, where the 100 classes are grouped into 20 superclasses, such as aquatic mammals, fish, flowers, and food containers. The dataset details can be found on the University of Toronto\u0026rsquo;s website.\nFig. 3. CiFAR-10 dataset. (Image source: University of Toronto). Vision Transformers (ViT) ViT is an encoder-based transformer neural network that uses a self-attention mechanism to transform the input image into fixed-size patches and encode their positions into the input, allowing the network to capture global features and locations. In contrast, convolutional neural networks (CNN) focus on extracting local features. The ViT architecture consists of the following steps: input, linear projection, stacked encoder, multi-layer perceptron, and output labels.\nFig. 4. Original vision transformers architecture. (Image source: Dosovitskiy, Alexey, et al.). The explanation of ViT is cited from the original paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\u0026quot; by Dosovitskiy, Alexey, et al. The image is first converted into patches, and each patch is reshaped into a 1D vector multiplied by a learnable matrix to create a vector. Next, positional embedding is integrated as ViT is invariant to position on patches. The encoder of ViT is copied directly from the original Transformer architecture. The embedded patches are passed through a Layer Normalization to reduce training time and stabilize the training phases. Then, we pass the information through a multi-head attention network added by a skip connection to improve the performance while reducing the risk of gradient explosion or vanishing. We then pass through another Layer Normalization, Multi-layer Perceptron, and skip connection for further processing. The use of positional embedding allows ViT to behave like CNN, and this is the only inductive bias in ViT. Compared with CNN, ViT enables the network to learn the global and abstract representations of the input image, making it more robust.\nJupyter Notebooks ▪ Single-Task Learning on CiFAR-10 Dataset: A Vision Transformer Approach. ▪ Single-Task Learning on CiFAR-10 Dataset - Super-Class Classfication: A Vision Transformer Approach. ▪ Single-Task Learning on CiFAR-100 Dataset: A Vision Transformer Approach. ▪ Single-Task Learning on CiFAR-100 Dataset - Super-Class Classfication: A Vision Transformer Approach. ▪ Multi-Task Learning on CiFAR-10 Dataset: A Vision Transformer Approach. ▪ Multi-Task Learning on CiFAR-100 Dataset: A Vision Transformer Approach. Deep Residual Network (ResNet-152) Deep residual network-152 layers, also known as ResNet-152, is a convolutional neural network architecture designed to address the vanishing gradient problem in building deep neural networks. It was introduced in the original paper \u0026ldquo;Deep residual learning for image recognition\u0026rdquo; by He, Kaiming, et al.\nFig. 5. Original ResNet architecture. (Image source: He, Kaiming, et al.). The key innovation of ResNet-152 is the residual block, which allows the network to learn residual functions that map the input to the output, rather than trying to learn the entire mapping in one shot. The residual block consists of two convolutional layers with batch normalization and ReLU activation, and a skip connection that adds the input of the block to the output of the second convolutional layer. Due to its depth, ResNet-152 can capture more complex features and achieve better performance than its shallower counterparts, such as ResNet-18, ResNet-34, and ResNet50. It has been used in various computer vision tasks, such as object detection, image classification, and image segmentation.\nJupyter Notebooks ▪ Single-Task Learning on CiFAR-10 Dataset (10 Classes): A ResNet-152 Approach. ▪ Single-Task Learning on CiFAR-10 Dataset (2 Classes): A ResNet-152 Approach. ▪ Single-Task Learning on CiFAR-100 Dataset (100 Classes): A ResNet-152 Approach. ▪ Single-Task Learning on CiFAR-100 Dataset (20 Superclasses): A ResNet-152 Approach. ▪ Mullti-task Learning on CiFAR-10 Dataset: A ResNet-152 Approach. ▪ Mullti-task Learning on CiFAR-100 Dataset: A ResNet-152 Approach. Experimental Results The experiment result summary can be found here.\nFrom the experimental result, we can answer the three research questions:\nRQ1: Is an MTL ViT model superior in performance to an MTL Convolution-based model (ResNet-152)?\nMTL ViT outperformed MTL ResNet-152 on CiFAR-100, while MTL ResNet-152 outperformed MTL ViT on CiFAR-10 regarding testing accuracies. This result suggests that the MTL ViT is better suited for complex classification tasks, as CiFAR-100 is a more complex dataset than CiFAR-10. With more complex datasets or longer training epochs, I expect MTL ViT to outperform MTL ResNet-152 on both datasets.\nRQ2: Can an MTL ViT model achieve better results than two separate STL ViT models?\nMTL ViT outperformed two STL ViTs on CiFAR-10 and CiFAR-100 regarding testing accuracies. This result aligns with previous studies on benchmark datasets such as Taskonomy, Replica, and CocoDoom. The superior performance of MTL ViT is due to the sharing of the same backbone between the two tasks, which enables the network to learn more representations while significantly reducing the number of parameters.\nRQ3: Does an STL ViT model outperform an STL Convolution-based model (ResNet152) in terms of accuracy?\nFor this question, there is no clear answer. If we use a more complex dataset or train with more epochs, we might see that ViT outperforms ResNet152 in CiFAR-10 and CiFAR-100. ViT can capture global features due to its positional embedding and attention mechanism, while ResNet-152 can capture local features due to the convolutional operation.\nCitation Cited as:\nNguyen, Minh. (March 2023). Multi-Task Learning for Image Classification. https://mnguyen0226.github.io/posts/multitask_learning/post/. Or\n@article{nguyen2023mtl, title = \u0026#34;Multi-Task Learning for Image Classification.\u0026#34;, author = \u0026#34;Nguyen, Minh\u0026#34;, journal = \u0026#34;mnguyen0226.github.io\u0026#34;, year = \u0026#34;2023\u0026#34;, month = \u0026#34;March\u0026#34;, url = \u0026#34;https://mnguyen0226.github.io/posts/multitask_learning/post/\u0026#34; } References [1] Y. Chen, J. Yu, Y. Zhao, J. Chen, and X. Du. Task’s choice: Pruning-based feature sharing (pbfs) for multi-task learning. Entropy, 24(3):432, 2022.\n[2] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3213–3223, 2016.\n[3] M. Crawshaw. Multi-task learning with deep neural networks: A survey. arXiv preprint arXiv:2009.09796, 2020.\n[4] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[5] Evannex. Andrej karpathy talks tesla autopilot amp; multitask learning, Aug 2019.\n[6] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.\n[7] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-10 and cifar100 datasets. URl: https://www. cs. toronto. edu/kriz/cifar.html, 6(1):1, 2009.\n[8] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740–755.Springer, 2014.\n[9] A. Mahendran, H. Bilen, J. F. Henriques, and A. Vedaldi. Researchdoom and cocodoom: Learning computer vision with games. arXiv preprint arXiv:1610.02431, 2016.\n[10] I. Misra, A. Shrivastava, A. Gupta, and M. Hebert. Crossstitch networks for multi-task learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3994–4003, 2016.\n[11] S. Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017.\n[12] J. Straub, T. Whelan, L. Ma, Y. Chen, E. Wijmans, S. Green, J. J. Engel, R. Mur-Artal, C. Ren, S. Verma, et al. The replica dataset: A digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019.\n[13] A. R. Zamir, A. Sax, W. Shen, L. J. Guibas, J. Malik, and S. Savarese. Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3712–3722, 2018.\n[14] X. Zheng, B. Wu, X. Zhu, and X. Zhu. Multi-task deep learning seismic impedance inversion optimization based on homoscedastic uncertainty. Applied Sciences, 12(3):1200, 2022.\nFig. 6. Sunset at Minneapolis, Minnesota, U.S.A. (Image source: Nicole Geri @ Unsplash). ","permalink":"https://mnguyen0226.github.io/posts/multitask_learning/post/","summary":"Status: [Latest]\nDuring a recent interview with a Data Science Lead at a digital agriculture tech company, I had the opportunity to delve into the machine learning team\u0026rsquo;s exciting project. The team was utilizing multi-task learning (MTL) to deploy models to farming hardware, which piqued my interest and prompted me to explore this approach combined with deep learning. In particular, I wanted to investigate the effectiveness of MTL on vision transformers (ViT) and deep residual networks (ResNet-152).","title":"Multi-Task Learning for Image Classification"},{"content":"👨‍💻 Experience Graduate Research Assistant @ Deloitte Touches x Commonwealth Cyber Initiatives ▪ Tech Stack: Python, Tensorflow, Matplotlib, Pandas, Seaborn. ▪ Date: 05/2022 - 08/2022. ▪ Location: Arlington, Virginia. Responsibilities ▪ Forecasted water levels and localized overflowed water tanks during storms by applying a temporal deep-learning model; reduced 25-30% total operation/maintenance costs and prevented pollution to DC's local rivers. ▪ Built an interactable explainable dashboard using the MDS algorithm for the literature database; provided new insights for stakeholders. ▪ Implemented TimeGAN; synthesized the dataset by 50%, improved prediction accuracy by 4%, and gained more data from stakeholders. Research Assistant @ Terrestrial Robotics Engineering \u0026 Controls Lab (Virginia Tech) ▪ Tech Stack: C#, C++, Unity. \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; ▪ Date: 09/2021 - 05/2022. ▪ Location: Blacksburg, Virginia. Responsibilities ▪ Improved transparency by applying ROS-Bridge data transfer pipeline between the low-level robotic controller and Unity. ▪ Created 3D interactive simulation environments in Unity with VR headsets and Haptix Gloves synchronization. ▪ Poster, Presentation. Machine Learning Intern @ Heron Systems (Shield AI Subsidiary) ▪ Tech Stack: Python, Javascript, PyTorch, Pandas, Plotly, SpaCy, Flask, MySQL. ▪ Date: 05/2021 - 08/2021. ▪ Location: Alexandria, Virginia. Responsibilities ▪ Coded custom neural networks for classifying army-winning probabilities in DARPA's Game Breaker program. ▪ Assured RNN/LSTM/GRU/Transformer-based models' performance before product deployment by developing NLP evaluation tests. ▪ Built a web dashboard showing army composition and deep learning model prediction results; contributed to the company's second round of funding from DARPA by providing demonstrative use cases of the dashboard in the final presentation and report. Research Assistant @ Geo Lab (William \u0026 Mary College) ▪ Tech Stack: Python, Tensorflow, Matplotlib. ▪ Date: 09/2020 - 05/2021. ▪ Location: Williamsburg, Virginia. Responsibilities ▪ Led a team of 5 to develop CNN models for road quality classification via satellite images, combined with AutoEncoder for data-poisoning defense tasks; won 3rd place (out of 8 competing universities) in model performance and contributions. ▪ Contributed 30% of the benchmark dataset by collecting and balancing classes with image augmentation techniques. Research Assistant @ Hybrid Electric Vehicle Team (Virginia Tech) ▪ Tech Stack: C++, MATLAB. \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; ▪ Date: 09/2020 - 05/2021. ▪ Location: Blacksburg, Virginia. Responsibilities ▪ Applied Sensor Fusion algorithm to Chevrolet Blazer 2019 by integrating/testing Borsh radar and Mobileye6 camera sensors. ▪ Implemented and tested the algorithm's performance in simulation; earned full points in the EcoCar Competition's road driving tests. Teaching Assistant @ Integrate System Design (Virginia Tech) ▪ Tech Stack: C++, Circuit Design, Arduino, MIT Mobile App Inventor. ▪ Date: 05/2020 - 05/2021. ▪ Location: Blacksburg, Virginia. Responsibilities ▪ Assisted instructors in grading assignments and mentoring 25 students in the semester-long Smart Home Simulation project. Research Assistant @ Wireless Lab (Virginia Tech) ▪ Tech Stack: C++, Python. ▪ Date: 05/2020 - 08/2020. ▪ Location: Blacksburg, Virginia. Responsibilities ▪ Designed a controller with two stepper motors using Python, Arduino, and GRBL library for long-range signal receiving or signal sweeping tasks. The user can control the Antenna's movements through their preferred angles by entering the angles in the Python scripted interface. 🔧 Skills Languages: Python, SQL, Java, Javascript, HTML, CSS. Web Development Tools: Flask, Vue.js, MySQL, SQLite, Git, Docker, Figma. Data Science Tools: Tensorflow, PyTorch, Scikit-learn, NumPy, Matplotlib, Pandas, Seaborn, Plotly, BeautifulSoup, OpenCV. 🎓 Education Master of Science in Computer Engineering ▪ Concentration: Software \u0026 Machine Intelligence. ▪ GPA: 3.82. ▪ Date: 08/2022 - 05/2024. ▪ Location: Arlington, Virginia. Bachelor of Science in Computer Engineering (Graduated) ▪ Concentration: Machine Learning, Computer Science Minor. ▪ GPA: 3.62. ▪ Date: 08/2018 - 05/2022. ▪ Location: Blacksburg, Virginia. Relevant Courses ▪ Deep Learning ▪ Web Application Development ▪ Software Engineering ▪ Data Visualization ▪ Advanced Machine Learning ▪ Trustworthy Machine Learning ▪ Data Analytics ▪ Computer Vision ▪ Digital Image Processing ▪ AI \u0026 Engineering Applications ▪ Real-time Systems ▪ Data Structure \u0026 Algorithms ▪ Principles Of Computer Architecture 📜 Publication \"DeepH20: Cyber attack detection in water distribution systems using deep learning.\" Nazmul Sikder, Minh T. Nguyen, Donald Elliot, Feras Batarseh. Elsevier's Journal of Water Process Engineering (Vol 52.) (04/2023). Link.\n\"AI for Cyberbiosecurity in Water Systems—A Survey.\" Daniel Sobien, Mehmet O. Yardimci, Minh T. Nguyen, Wan-Yi Mao, Vinita Fordham, Abdul Rahman, Susan Duncan, Feras Batarseh. Springer's Cyberbiosecurity Book (01/2023). Link.\n💻 Projects Mathbridge ▪ Tech Stack: Python, Dash, Bootstrap, Docker, Kubernetes, HTML, CSS. \u0026nbsp; \u0026nbsp; ▪ Developed a visualization webapp for Virginia Tech's Computer Science Department as an machine learning algorithm explanation and playground . ▪ Link. Books.JBP ▪ Tech Stack: Java, Vue.js, MySQL, Figma, HTML, CSS. \u0026nbsp; \u0026nbsp; ▪ Developed a single-page full-stack e-commerce web app; followed DAO pattern and SOLID principles. ▪ Github. Flask-Y ▪ Tech Stack: Python, Javascript, MySQL, Flask API, Werkzeug API, CKEditor API, Bootstrap, HTML, CSS. \u0026nbsp; \u0026nbsp; ▪ Redesigned Y Combinator's Hacker News to be more user-friendly (Reddit style). ▪ Developed a multi-page full-stack media web app that allows multiple users to sign-up, login, and manage/comment/vote posts. ▪ Github. Smart Home Simulation ▪ Tech Stack: C++, Circuit Design, Arduino, MIT Mobile App Inventor. \u0026nbsp; \u0026nbsp; \u0026nbsp; \u0026nbsp; ▪ Designed, built, and tested hardware simulation with 8 automated sensors, controlled wirelessly via an Android mobile app. ▪ Appointed to class's teaching assistant by professor due to project's design, presentation, report, and assistance to classmates. ▪ Github. 🏅 Awards Best ML Poster @ FLAIRS-35 ▪ Best journal poster for ML applications in cyberbiosecurity, awarded by juries amongst 40+ submissions. ▪ Link. Turing Research Fellowship @ Commonwealth Cyber Initiatives ▪ Awarded AI Fellowship out of 21 universities in the Commonwealth of Virginia state. ▪ Link. Geo Research Fellowship @ Geo Lab ▪ One of 30 fellows (out of 150 applicants) for the US's largest geospatial data security lab. ▪ Link. 🙌 Volunteers President @ Teaching Robotics \u0026 Engineering Club (Virginia Tech) ▪ Taught C/C++, Arduino, and electric foundations and robotics projects for 15-25 club members. ▪ Supervised club Officers in making teaching materials and mentoring club members. Industry Relations Chair @ IEEE Student Branch (Virginia Tech) ▪ Tripled the number of participants by hosting peer networking events and info sessions to connect students to faculty-sponsored and company-sponsored opportunities in the ECE department. ▪ Collaborated with company representatives in IEEE@VT Summit, resume review sessions, and tech talks. ▪ Raised a $6,000 annual sponsorship from Collins Aerospace, Lockheed Martin, Boeing, and Texas Instruments. Student Mentor @ Center for Enhancement of Engineering Diversity (Virginia Tech) ▪ Served as a sounding board for various issues that confront first-year students during the first 10 weeks. ▪ Held weekly meetings to provide 10 mentees about how to smoothly transition into Virginia Tech culture. Officer @ IEEE Student Branch (Virginia Tech) ▪ Organized “Fun Friday’s” peer networking events and the IEEE Summit industry/leadership conference. Captain @ Math Team (St. Paul Preparatory School) ▪ Won Team Second Place in the 2017 Minnesota High School Mathematics League Tournament. ▪ Assisted the teacher with explaining difficult problems to members. ▪ Assigned math areas to different teammates based on skill sets to boost the team's total score. ","permalink":"https://mnguyen0226.github.io/about/","summary":"👨‍💻 Experience Graduate Research Assistant @ Deloitte Touches x Commonwealth Cyber Initiatives ▪ Tech Stack: Python, Tensorflow, Matplotlib, Pandas, Seaborn. ▪ Date: 05/2022 - 08/2022. ▪ Location: Arlington, Virginia. Responsibilities ▪ Forecasted water levels and localized overflowed water tanks during storms by applying a temporal deep-learning model; reduced 25-30% total operation/maintenance costs and prevented pollution to DC's local rivers. ▪ Built an interactable explainable dashboard using the MDS algorithm for the literature database; provided new insights for stakeholders.","title":"About Me"}]